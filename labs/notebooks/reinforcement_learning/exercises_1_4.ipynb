{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Value Function\n",
    "\n",
    "We want to calculate $V_{\\pi}(s)$ (the state-value-function given a policy)\n",
    "![mdp.png](mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Policy Evaluation by Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n",
      "[10.  2.  3.]\n",
      "[10.49  2.61  3.85]\n",
      "[10.5594  2.6674  3.9038]\n",
      "[10.56532   2.673704  3.910464]\n",
      "[10.56595688  2.6743188   3.91106728]\n",
      "[10.56601845  2.67438127  3.91113041]\n",
      "[10.5660247   2.67438748  3.91113659]\n",
      "[10.56602532  2.6743881   3.91113722]\n",
      "[10.56602538  2.67438816  3.91113728]\n",
      "[10.56602539  2.67438817  3.91113728]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n",
      "[10.56602539  2.67438817  3.91113729]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "policy=np.array([[0.3, 0.2, 0.5], [0.5, 0.4, 0.1], [0.8, 0.1, 0.1]])\n",
    "# 'raw_rewards' variable contains rewards obtained after transition to each state\n",
    "# In our example it doesn't depend on source state\n",
    "raw_rewards = np.array([1.5, -1.833333333, 19.833333333])\n",
    "# 'rewards' variable contains expected values of the next reward for each state\n",
    "rewards = np.matmul(policy, raw_rewards)\n",
    "assert np.allclose(rewards, np.array([10., 2., 3.]))\n",
    "\n",
    "state_value_function=np.array([0 for i in range(3)])\n",
    "\n",
    "gamma = 0.1\n",
    "for i in range(20):\n",
    "    print(state_value_function)\n",
    "    state_value_function = rewards + gamma * np.matmul(policy, state_value_function)\n",
    "    \n",
    "print(state_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6.1 Policy Evaluation by Linear Programming\n",
    "\n",
    "The state-value-function can be directly solved through linear programming (as shown on page 15):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.56602539  2.67438817  3.91113729]\n"
     ]
    }
   ],
   "source": [
    "solution =np.linalg.inv(np.eye(policy.shape[0]) - gamma*policy)\n",
    "solution = np.matmul(solution, rewards)\n",
    "#TODO: Implement the linear programming solution with a discount rate of 0.1\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Monte Carlo Policy Evaluation\n",
    "\n",
    "\n",
    "Monte Carlo Policy Evaluation can also be used, whereby sampling is used to get to the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.56473158  2.66801647  3.90664402]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "reward_counter=np.array([0., 0., 0.])\n",
    "visit_counter=np.array([0., 0., 0.])\n",
    "\n",
    "def gt(rewardlist, gamma=0.1):\n",
    "    '''\n",
    "    Function to calculate the total discounted reward\n",
    "    >>> gt([10, 2, 3], gamma=0.1)\n",
    "    10.23\n",
    "    '''\n",
    "    rewards = np.array(rewardlist)\n",
    "    powers = np.arange(rewards.shape[0])\n",
    "    powers = np.power(gamma*np.ones(rewards.shape[0]), powers)\n",
    "    #print(rewards)\n",
    "    #print(powers)\n",
    "    return np.sum(np.multiply(rewards, powers))\n",
    "\n",
    "\n",
    "for i in range(400):\n",
    "    start_state=random.randint(0, 2)\n",
    "    next_state=start_state\n",
    "    rewardlist=[]\n",
    "    occurence=defaultdict(list) \n",
    "    for i in range(250):\n",
    "        rewardlist.append(rewards[next_state]) \n",
    "        occurence[next_state].append(len(rewardlist)-1) \n",
    "        action=np.random.choice(np.arange(0, 3), p=policy[next_state]) \n",
    "        next_state=action\n",
    "\n",
    "    for state in occurence: \n",
    "        for value in occurence[state]: \n",
    "            rew=gt(rewardlist[value:]) \n",
    "            reward_counter[state]+=rew \n",
    "            visit_counter[state]+=1 \n",
    "            #break #if break: return following only the first visit\n",
    "print(reward_counter/visit_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As can be seen the result is nearly the same as the state-value-function calculated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Policy Optimization by Q-Learning\n",
    "\n",
    "This code solves a very easy problem: using the rewards it calculated the optimal action-value-function.\n",
    "\n",
    "It samples a state-action pair randomly, so that all state-action pairs can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.7037037   0.37037037 22.03703704]\n",
      " [ 3.7037037   0.37037037 22.03703704]\n",
      " [ 3.7037037   0.37037037 22.03703704]]\n"
     ]
    }
   ],
   "source": [
    "q_table=np.zeros((3, 3)) \n",
    "alfa = 0.99\n",
    "gamma = 0.1\n",
    "for i in range(1001): \n",
    "    state=random.randint(0, 2) \n",
    "    action=random.randint(0, 2) \n",
    "    next_state=action\n",
    "    reward=raw_rewards[next_state] \n",
    "    next_q=max(q_table[next_state]) \n",
    "    q_table[state, action]= (1.0 - alfa) * q_table[state, action] +  alfa * (reward + gamma * next_q)\n",
    "    #if i%100==0:\n",
    "        #print(q_table)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.2037037,  4.2037037,  5.2037037])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_s = np.zeros(3)\n",
    "for state in range(3):\n",
    "    for action in range(3):\n",
    "        value_s[state] = value_s[state] + q_table[state, action] * policy[state, action]\n",
    "value_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Score Function Gradient Estimator\n",
    "Implement the score function gradient estimator in lxmls/reinforcement_learning/score\\_function\\_estimator.py. Check it is correct by calling the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/candeiasalexandre/code/lxmls-toolkit/lxmls/reinforcement_learning/score_function_estimator.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  poli = torch.nn.functional.softmax(model.t_policy).data.numpy()\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Exercise 6.3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-06701649b203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlxmls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinforcement_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_function_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/lxmls-toolkit/lxmls/reinforcement_learning/score_function_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Solution to Exercise 6.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exercise 6.3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# code needed at this identation level!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Exercise 6.3"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from lxmls.reinforcement_learning.score_function_estimator import train\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Value Iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards=np.array([10., 2., 3.])\n",
    "gamma = 0.1\n",
    "\n",
    "state_value_function = np.zeros(3)\n",
    "\n",
    "for i in range(1000):\n",
    "    for s in range(3):\n",
    "        state_value_function[s]=#TODO: Implement the state value function update\n",
    "print(state_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Policy Gradient for the CartPole task\n",
    "Implement policy gradient for the cartpole task by coding the forward pass of Model() in lxmls/reinforcement\\_learning/policy\\_gradient.py. Check it is correct by calling the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.reinforcement_learning.policy_gradient import train\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Actor Critic for the CartPole task\n",
    "Implement actor crtitic for the cartpole task by coding the critic forward pass in lxmls/reinforcement\\_learning/policy\\_gradient.py. Check it is correct by calling the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.reinforcement_learning.actor_critic import train\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
